{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Random Forest Regressor\n",
    "# rf = RandomForestRegressor(random_state=42)\n",
    "rf = RandomForestRegressor(max_depth=20, max_features='log2', min_samples_leaf=4, min_samples_split=10, n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_predictions = rf.predict(X_test)\n",
    "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
    "rf_mae = mean_absolute_error(y_test, rf_predictions)\n",
    "\n",
    "print(\"Random Forest MSE:\", rf_mse)\n",
    "print(\"Random Forest MAE:\", rf_mae)\n",
    "\n",
    "rf_r2_train = r2_score(y_train, rf.predict(X_train))\n",
    "rf_r2_test = r2_score(y_test, rf_predictions)\n",
    "\n",
    "print(\"Random Forest R2 - Train:\", rf_r2_train)\n",
    "print(\"Random Forest R2 - Test:\", rf_r2_test)\n",
    "\n",
    "rf_rmse = np.sqrt(rf_mse)\n",
    "print(\"Random Forest RMSE:\", rf_rmse)\n",
    "\n",
    "# k-cross validation\n",
    "accuracies = cross_val_score(estimator=rf, X=X_train, y=y_train, cv=5)\n",
    "print(\"The mean training accuracy is: %.3f\" % accuracies.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# XGBoost\n",
    "# xgb_reg = xgb.XGBRegressor()\n",
    "xgb_reg = xgb.XGBRegressor(alpha=0.1, colsample_bytree=1.0, gamma=0.2, learning_rate=0.3, max_bin=256, max_depth=7, min_child_weight=5, n_estimators=200)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "xgb_predictions = xgb_reg.predict(X_test)\n",
    "xgb_mse = mean_squared_error(y_test, xgb_predictions)\n",
    "xgb_mae = mean_absolute_error(y_test, xgb_predictions)\n",
    "\n",
    "print(\"XGBoost MSE:\", xgb_mse)\n",
    "print(\"XGBoost MAE:\", xgb_mae)\n",
    "\n",
    "xgb_r2_train = r2_score(y_train, xgb_reg.predict(X_train))\n",
    "xgb_r2_test = r2_score(y_test, xgb_predictions)\n",
    "\n",
    "print(\"XGBoost R2 - Train:\", xgb_r2_train)\n",
    "print(\"XGBoost R2 - Test:\", xgb_r2_test)\n",
    "\n",
    "xgb_rmse = np.sqrt(xgb_mse)\n",
    "print(\"XGBoost RMSE:\", xgb_rmse)\n",
    "\n",
    "# k-cross validation\n",
    "accuracies = cross_val_score(estimator=xgb_reg, X=X_train, y=y_train, cv=5)\n",
    "print(\"The mean training accuracy is: %.3f\" % accuracies.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# LightGBM\n",
    "# lgb_reg = lgb.LGBMRegressor()\n",
    "lgb_reg = lgb.LGBMRegressor(learning_rate=0.1, max_depth=10, min_child_samples=20, n_estimators=100, num_leaves=10, random_state=42, reg_alpha=0.1, reg_lambda=0.1)\n",
    "\n",
    "lgb_reg.fit(X_train, y_train)\n",
    "lgb_predictions = lgb_reg.predict(X_test)\n",
    "lgb_mse = mean_squared_error(y_test, lgb_predictions)\n",
    "lgb_mae = mean_absolute_error(y_test, lgb_predictions)\n",
    "\n",
    "print(\"LightGBM MSE:\", lgb_mse)\n",
    "print(\"LightGBM MAE:\", lgb_mae)\n",
    "\n",
    "lgb_r2_train = r2_score(y_train, lgb_reg.predict(X_train))\n",
    "lgb_r2_test = r2_score(y_test, lgb_predictions)\n",
    "\n",
    "print(\"LightGBM R2 - Train:\", lgb_r2_train)\n",
    "print(\"LightGBM R2 - Test:\", lgb_r2_test)\n",
    "\n",
    "lgb_rmse = np.sqrt(lgb_mse)\n",
    "print(\"LightGBM RMSE:\", lgb_rmse)\n",
    "\n",
    "# k-cross validation\n",
    "accuracies = cross_val_score(estimator=lgb_reg, X=X_train, y=y_train, cv=5)\n",
    "print(\"The mean training accuracy is: %.3f\" % accuracies.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# SVR\n",
    "# svr = SVR()\n",
    "svr = SVR(C=1.0, epsilon=0.1, kernel='rbf')\n",
    "svr.fit(X_train, y_train)\n",
    "svr_predictions = svr.predict(X_test)\n",
    "svr_mse = mean_squared_error(y_test, svr_predictions)\n",
    "svr_mae = mean_absolute_error(y_test, svr_predictions)\n",
    "\n",
    "print(\"SVR MSE:\", svr_mse)\n",
    "print(\"SVR MAE:\", svr_mae)\n",
    "\n",
    "svr_r2_train = r2_score(y_train, svr.predict(X_train))\n",
    "svr_r2_test = r2_score(y_test, svr_predictions)\n",
    "\n",
    "print(\"SVR R2 - Train:\", svr_r2_train)\n",
    "print(\"SVR R2 - Test:\", svr_r2_test)\n",
    "\n",
    "svr_rmse = np.sqrt(svr_mse)\n",
    "print(\"SVR RMSE:\", svr_rmse)\n",
    "\n",
    "# k-cross validation\n",
    "accuracies = cross_val_score(estimator=svr, X=X_train, y=y_train, cv=5)\n",
    "print(\"The mean training accuracy is: %.3f\" % accuracies.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Neural Network Regressor with two hidden layers, each containing 30 neurons\n",
    "nn = MLPRegressor(activation = \"relu\", alpha = 0.1, hidden_layer_sizes = (30, 30), solver = 'lbfgs', random_state=42)\n",
    "\n",
    "nn.fit(X_train, y_train)\n",
    "nn_predictions = nn.predict(X_test)\n",
    "nn_mse = mean_squared_error(y_test, nn_predictions)\n",
    "nn_mae = mean_absolute_error(y_test, nn_predictions)\n",
    "\n",
    "print(\"Neural Network MSE:\", nn_mse)\n",
    "print(\"Neural Network MAE:\", nn_mae)\n",
    "\n",
    "nn_r2_train = r2_score(y_train, nn.predict(X_train))\n",
    "nn_r2_test = r2_score(y_test, nn_predictions)\n",
    "\n",
    "print(\"Neural Network R2 - Train:\", nn_r2_train)\n",
    "print(\"Neural Network R2 - Test:\", nn_r2_test)\n",
    "\n",
    "nn_rmse = np.sqrt(nn_mse)\n",
    "print(\"Neural Network RMSE:\", nn_rmse)\n",
    "\n",
    "# k-cross validation\n",
    "accuracies = cross_val_score(estimator=nn, X=X_train, y=y_train, cv=5)\n",
    "print(\"The mean training accuracy is: %.3f\" % accuracies.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(max_depth=20, max_features='log2', min_samples_leaf=4, min_samples_split=10, n_estimators=200, random_state=42)\n",
    "\n",
    "# Train the model on the entire training data\n",
    "rf.fit(X_train, y_train)\n",
    "rf_predictions = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, rf_predictions)\n",
    "precision = precision_score(y_test, rf_predictions, average='macro')\n",
    "recall = recall_score(y_test, rf_predictions, average='macro')\n",
    "f1 = f1_score(y_test, rf_predictions, average='macro')\n",
    "\n",
    "print(\"Random Forest Accuracy:\", accuracy)\n",
    "print(\"Random Forest Precision:\", precision)\n",
    "print(\"Random Forest Recall:\", recall)\n",
    "print(\"Random Forest F1 Score:\", f1)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_accuracy = cross_val_score(estimator=rf, X=X_train, y=y_train, cv=5, scoring='accuracy')\n",
    "cv_precision = cross_val_score(estimator=rf, X=X_train, y=y_train, cv=5, scoring='precision_macro')\n",
    "cv_recall = cross_val_score(estimator=rf, X=X_train, y=y_train, cv=5, scoring='recall_macro')\n",
    "cv_f1 = cross_val_score(estimator=rf, X=X_train, y=y_train, cv=5, scoring='f1_macro')\n",
    "\n",
    "print(\"Cross-Validation Accuracy:\", np.mean(cv_accuracy))\n",
    "print(\"Cross-Validation Precision:\", np.mean(cv_precision))\n",
    "print(\"Cross-Validation Recall:\", np.mean(cv_recall))\n",
    "print(\"Cross-Validation F1 Score:\", np.mean(cv_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Support Vector Classifier (SVC)\n",
    "# svc = SVC()\n",
    "svc = SVC(C=10, kernel='poly', random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "svc_predictions = svc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, svc_predictions)\n",
    "precision = precision_score(y_test, svc_predictions, average='macro')\n",
    "recall = recall_score(y_test, svc_predictions, average='macro')\n",
    "f1 = f1_score(y_test, svc_predictions, average='macro')\n",
    "\n",
    "print(\"SVC Accuracy:\", accuracy)\n",
    "print(\"SVC Precision:\", precision)\n",
    "print(\"SVC Recall:\", recall)\n",
    "print(\"SVC F1 Score:\", f1)\n",
    "\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_accuracy = cross_val_score(estimator=svc, X=X_train, y=y_train, cv=5, scoring='accuracy')\n",
    "cv_precision = cross_val_score(estimator=svc, X=X_train, y=y_train, cv=5, scoring='precision_macro')\n",
    "cv_recall = cross_val_score(estimator=svc, X=X_train, y=y_train, cv=5, scoring='recall_macro')\n",
    "cv_f1 = cross_val_score(estimator=svc, X=X_train, y=y_train, cv=5, scoring='f1_macro')\n",
    "\n",
    "print(\"Cross-Validation Accuracy:\", np.mean(cv_accuracy))\n",
    "print(\"Cross-Validation Precision:\", np.mean(cv_precision))\n",
    "print(\"Cross-Validation Recall:\", np.mean(cv_recall))\n",
    "print(\"Cross-Validation F1 Score:\", np.mean(cv_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# LightGBM Classifier\n",
    "# lgb_clf = lgb.LGBMClassifier()\n",
    "lgb_clf = lgb.LGBMClassifier(learning_rate=0.3, max_depth=7, n_estimators=200, num_leaves=63, random_state=42)\n",
    "lgb_clf.fit(X_train, y_train)\n",
    "lgb_predictions = lgb_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, lgb_predictions)\n",
    "precision = precision_score(y_test, lgb_predictions, average='macro')\n",
    "recall = recall_score(y_test, lgb_predictions, average='macro')\n",
    "f1 = f1_score(y_test, lgb_predictions, average='macro')\n",
    "\n",
    "print(\"LightGBM Accuracy:\", accuracy)\n",
    "print(\"LightGBM Precision:\", precision)\n",
    "print(\"LightGBM Recall:\", recall)\n",
    "print(\"LightGBM F1 Score:\", f1)\n",
    "\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_accuracy = cross_val_score(estimator=lgb_clf, X=X_train, y=y_train, cv=5, scoring='accuracy')\n",
    "cv_precision = cross_val_score(estimator=lgb_clf, X=X_train, y=y_train, cv=5, scoring='precision_macro')\n",
    "cv_recall = cross_val_score(estimator=lgb_clf, X=X_train, y=y_train, cv=5, scoring='recall_macro')\n",
    "cv_f1 = cross_val_score(estimator=lgb_clf, X=X_train, y=y_train, cv=5, scoring='f1_macro')\n",
    "\n",
    "print(\"Cross-Validation Accuracy:\", np.mean(cv_accuracy))\n",
    "print(\"Cross-Validation Precision:\", np.mean(cv_precision))\n",
    "print(\"Cross-Validation Recall:\", np.mean(cv_recall))\n",
    "print(\"Cross-Validation F1 Score:\", np.mean(cv_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# XGBoost Classifier\n",
    "# xgb_clf = xgb.XGBClassifier()\n",
    "xgb_clf = xgb.XGBClassifier(alpha=0.1, booster='gbtree', colsample_bytree=1.0, gamma=0.2, learning_rate=0.3, max_depth=7, n_estimators=200,  objective='multi:softprob')\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "xgb_predictions = xgb_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, xgb_predictions)\n",
    "precision = precision_score(y_test, xgb_predictions, average='macro')\n",
    "recall = recall_score(y_test, xgb_predictions, average='macro')\n",
    "f1 = f1_score(y_test, xgb_predictions, average='macro')\n",
    "\n",
    "print(\"XGBoost Accuracy:\", accuracy)\n",
    "print(\"XGBoost Precision:\", precision)\n",
    "print(\"XGBoost Recall:\", recall)\n",
    "print(\"XGBoost F1 Score:\", f1)\n",
    "\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_accuracy = cross_val_score(estimator=xgb_clf, X=X_train, y=y_train, cv=5, scoring='accuracy')\n",
    "cv_precision = cross_val_score(estimator=xgb_clf, X=X_train, y=y_train, cv=5, scoring='precision_macro')\n",
    "cv_recall = cross_val_score(estimator=xgb_clf, X=X_train, y=y_train, cv=5, scoring='recall_macro')\n",
    "cv_f1 = cross_val_score(estimator=xgb_clf, X=X_train, y=y_train, cv=5, scoring='f1_macro')\n",
    "\n",
    "print(\"Cross-Validation Accuracy:\", np.mean(cv_accuracy))\n",
    "print(\"Cross-Validation Precision:\", np.mean(cv_precision))\n",
    "print(\"Cross-Validation Recall:\", np.mean(cv_recall))\n",
    "print(\"Cross-Validation F1 Score:\", np.mean(cv_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# MLPClassifier\n",
    "nn = MLPClassifier(hidden_layer_sizes=(30, 30), activation='relu', alpha=0.1, solver='lbfgs', random_state=42)\n",
    "nn.fit(X_train, y_train)\n",
    "nn_predictions = nn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, nn_predictions)\n",
    "precision = precision_score(y_test, nn_predictions, average='macro')\n",
    "recall = recall_score(y_test, nn_predictions, average='macro')\n",
    "f1 = f1_score(y_test, nn_predictions, average='macro')\n",
    "\n",
    "print(\"Neural Network Accuracy:\", accuracy)\n",
    "print(\"Neural Network Precision:\", precision)\n",
    "print(\"Neural Network Recall:\", recall)\n",
    "print(\"Neural Network F1 Score:\", f1)\n",
    "\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_accuracy = cross_val_score(estimator=nn, X=X_train, y=y_train, cv=5, scoring='accuracy')\n",
    "cv_precision = cross_val_score(estimator=nn, X=X_train, y=y_train, cv=5, scoring='precision_macro')\n",
    "cv_recall = cross_val_score(estimator=nn, X=X_train, y=y_train, cv=5, scoring='recall_macro')\n",
    "cv_f1 = cross_val_score(estimator=nn, X=X_train, y=y_train, cv=5, scoring='f1_macro')\n",
    "\n",
    "print(\"Cross-Validation Accuracy:\", np.mean(cv_accuracy))\n",
    "print(\"Cross-Validation Precision:\", np.mean(cv_precision))\n",
    "print(\"Cross-Validation Recall:\", np.mean(cv_recall))\n",
    "print(\"Cross-Validation F1 Score:\", np.mean(cv_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
